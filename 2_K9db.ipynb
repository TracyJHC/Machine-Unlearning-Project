{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoTtqxa1Bmi_"
      },
      "source": [
        "# Machine Unlearning: Retraining and Scrubbing in a K9db-Integrated System\n",
        "Final Project for DS 593 Fall 2025\n",
        "\n",
        "Tracy Cui, Yuki Li, Yang Lu, Xin Wei\n",
        "\n",
        "## Code Notebook 2: K9db Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import copy\n",
        "from tabulate import tabulate\n",
        "\n",
        "import sqlite3\n",
        "import random\n",
        "from contextlib import closing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "474d6de0"
      },
      "outputs": [],
      "source": [
        "# Depends on if running this notebook separately, we might need to redefine the parameters\n",
        "if 'CONFIG' not in globals():\n",
        "    CONFIG = {\n",
        "        \"total_users\": 100,\n",
        "        \"batch_size\": 128,\n",
        "        \"seed\": 42,\n",
        "        \"zipf_param\": 1.5,\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    }\n",
        "\n",
        "class K9dbMock:\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\">>> [K9db] Booting up K9db SQL Backend for MNIST...\")\n",
        "\n",
        "        # 1. Setup SQLite in Memory\n",
        "        self.db = sqlite3.connect(\":memory:\")\n",
        "        self.cursor = self.db.cursor()\n",
        "\n",
        "        # Enable Foreign Keys for Cascade Logic\n",
        "        self.cursor.execute(\"PRAGMA foreign_keys = ON;\")\n",
        "\n",
        "        # Create Schema: Users -> Images\n",
        "        self.cursor.execute(\"CREATE TABLE users (user_id INTEGER PRIMARY KEY)\")\n",
        "        self.cursor.execute(\"\"\"\n",
        "            CREATE TABLE images (\n",
        "                global_index INTEGER PRIMARY KEY,\n",
        "                owner_id INTEGER,\n",
        "                FOREIGN KEY (owner_id) REFERENCES users(user_id) ON DELETE CASCADE\n",
        "            )\n",
        "        \"\"\")\n",
        "        self.cursor.execute(\"CREATE INDEX idx_owner ON images(owner_id)\")\n",
        "        self.db.commit()\n",
        "\n",
        "        # 2. Load MNIST Data\n",
        "        print(\">>> [K9db] Ingesting MNIST Dataset...\")\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,)) # MNIST Mean/Std\n",
        "        ])\n",
        "        self.full_mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "        # 3. Distribute data using Zipfian (Power Law) among users\n",
        "        print(\">>> [K9db] Distributing data using Zipfian (Power Law) among users...\")\n",
        "        num_items = len(self.full_mnist_dataset)\n",
        "        total_users = CONFIG[\"total_users\"]\n",
        "        zipf_param = CONFIG[\"zipf_param\"]\n",
        "\n",
        "        # Generate Zipfian probabilities\n",
        "        ranks = np.arange(1, total_users + 1)\n",
        "        weights = 1 / np.power(ranks, zipf_param)\n",
        "        weights /= weights.sum() # Normalize to sum to 1\n",
        "\n",
        "        # Assign counts per user\n",
        "        # Ensure sum of counts equals num_items\n",
        "        counts = np.random.multinomial(num_items, weights)\n",
        "\n",
        "        # Create mapping of global_index to owner_id\n",
        "        all_indices = np.arange(num_items)\n",
        "        np.random.shuffle(all_indices)\n",
        "\n",
        "        current_idx_in_all_indices = 0\n",
        "        user_rows = []\n",
        "        image_rows = []\n",
        "        self.data_map = {}\n",
        "\n",
        "        for user_id, count in enumerate(counts):\n",
        "            user_rows.append((user_id,))\n",
        "            user_assigned_indices = []\n",
        "            if count > 0:\n",
        "                # Assign 'count' images to this user\n",
        "                # Ensure we don't go out of bounds if multinomial distribution makes last user have more than available indices\n",
        "                end_idx = min(current_idx_in_all_indices + count, num_items)\n",
        "                user_indices = all_indices[current_idx_in_all_indices : end_idx]\n",
        "                for img_idx in user_indices:\n",
        "                    image_rows.append((int(img_idx), int(user_id)))\n",
        "                    user_assigned_indices.append(int(img_idx))\n",
        "                current_idx_in_all_indices += len(user_indices)\n",
        "            self.data_map[user_id] = user_assigned_indices\n",
        "\n",
        "        self.cursor.executemany(\"INSERT INTO users (user_id) VALUES (?)\", user_rows)\n",
        "        self.cursor.executemany(\"INSERT INTO images (global_index, owner_id) VALUES (?, ?)\", image_rows)\n",
        "        self.db.commit()\n",
        "\n",
        "        # Final check for data distribution\n",
        "        total_images_in_db = self.cursor.execute(\"SELECT COUNT(*) FROM images\").fetchone()[0]\n",
        "        print(f\">>> [K9db] Indexing Complete. {total_images_in_db} images mapped to {total_users} owners.\")\n",
        "\n",
        "    def get_full_dataset(self):\n",
        "        \"\"\"\n",
        "        Returns the original full MNIST dataset.\n",
        "        \"\"\"\n",
        "        return self.full_mnist_dataset\n",
        "\n",
        "    def get_data_map(self):\n",
        "        \"\"\"\n",
        "        Returns the data_map dictionary for user-to-image index mapping.\n",
        "        \"\"\"\n",
        "        return self.data_map\n",
        "\n",
        "    def get_loaders(self):\n",
        "        \"\"\"\n",
        "        Returns loaders for the CURRENT valid state.\n",
        "        Queries the DB to see which images are still 'live'.\n",
        "        \"\"\"\n",
        "        # Select all indices that currently exist in the DB\n",
        "        self.cursor.execute(\"SELECT global_index FROM images\")\n",
        "        valid_indices = [row[0] for row in self.cursor.fetchall()]\n",
        "\n",
        "        # Create PyTorch Subset\n",
        "        train_sub = Subset(self.full_mnist_dataset, valid_indices)\n",
        "\n",
        "        train_loader = DataLoader(train_sub, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
        "\n",
        "        return train_loader\n",
        "\n",
        "    def execute_delete_command(self, user_id):\n",
        "        \"\"\"\n",
        "        Simulates: DELETE FROM Users WHERE id = {user_id};\n",
        "        K9db Logic:\n",
        "           1. We identify the user.\n",
        "           2. We delete the user.\n",
        "           3. The database ENGINE automatically deletes the images.\n",
        "        \"\"\"\n",
        "        print(f\"\\n>>> [K9db SQL EXEC] DELETE FROM users WHERE user_id = {user_id};\")\n",
        "\n",
        "        # 1. Pre-fetch Forget Set (simulating the identification step)\n",
        "        self.cursor.execute(\"SELECT global_index FROM images WHERE owner_id = ?\", (user_id,))\n",
        "        forget_indices = [row[0] for row in self.cursor.fetchall()]\n",
        "\n",
        "        if not forget_indices:\n",
        "            print(f\">>> [K9db] Warning: User {user_id} not found or has no data currently associated.\")\n",
        "            # Return empty lists if user has no data or doesn't exist\n",
        "            return [], self.cursor.execute(\"SELECT global_index FROM images\").fetchall()\n",
        "\n",
        "        # 2. Execute SQL Delete (Triggers Cascade)\n",
        "        self.cursor.execute(\"DELETE FROM users WHERE user_id = ?\", (user_id,))\n",
        "        self.db.commit()\n",
        "\n",
        "        # 3. Fetch 'Retain Set' (Everything remaining)\n",
        "        self.cursor.execute(\"SELECT global_index FROM images\")\n",
        "        \n",
        "        # Fetchall returns list of tuples, convert to list of ints\n",
        "        retain_indices = [row[0] for row in self.cursor.fetchall()]\n",
        "\n",
        "        print(f\">>> [K9db] Cascade successful. Purged {len(forget_indices)} images related to user {user_id}.\")\n",
        "\n",
        "        # Return purely the indices so the pipeline can do its math\n",
        "        return forget_indices, retain_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acdca0f8",
        "outputId": "8fd0e4c9-342f-4ece-ea02-5d3ecec171d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> [K9db] Booting up K9db SQL Backend for MNIST...\n",
            ">>> [K9db] Ingesting MNIST Dataset...\n",
            ">>> [K9db] Distributing data using Zipfian (Power Law) among users...\n",
            ">>> [K9db] Indexing Complete. 60000 images mapped to 100 owners.\n",
            "Total images in full_dataset: 60000\n",
            "Number of users in data_map: 100\n"
          ]
        }
      ],
      "source": [
        "# Initialize K9db\n",
        "db = K9dbMock()\n",
        "\n",
        "# Get the full dataset and data_map from K9dbMock for consistency\n",
        "full_dataset = db.get_full_dataset()\n",
        "data_map = db.get_data_map()\n",
        "\n",
        "print(f\"Total images in full_dataset: {len(full_dataset)}\")\n",
        "print(f\"Number of users in data_map: {len(data_map)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY-mYVWyJxuo"
      },
      "source": [
        "then we tried MNIST, but assign each user with a specific unique set. Like user 1 is associated with all images with number 0."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
