{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoTtqxa1Bmi_"
      },
      "source": [
        "# Machine Unlearning: Retraining and Scrubbing in a K9db-Integrated System\n",
        "Final Project for DS 593 Fall 2025\n",
        "\n",
        "Tracy Cui, Yuki Li, Yang Lu, Xin Wei\n",
        "\n",
        "## Code Notebook 3: Deletion with Exclusive Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY-mYVWyJxuo"
      },
      "source": [
        "Here we implemented the models on MNIST again, but assigned each user with a specific unique set. For example, user 1 is associated with all images with number 0. Code annotation is skipped for those code shared between notebooks 1 and 3 and we focused on explaining the changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4Dthk3OJ8OQ",
        "outputId": "3d1b5ea3-b23a-4e17-fdcb-96da273d9916"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on device: cuda\n",
            "Loading MNIST...\n",
            "Constructing Class-Split (User 0 = Digit 0)...\n",
            "User 0 (Forget Target) has 5923 images (All Zeros)\n",
            "Other Users share 54077 images (Digits 1-9)\n",
            "\n",
            "--- [Phase 1] Generating Table 1 ---\n",
            "Training Base Model (M_All)...\n",
            "Running Retrain Benchmark (M0 - No Zeros)...\n",
            "Running Fisher Scrub (M2)...\n",
            "\n",
            "+-----------------------------+---------------------+---------------------+\n",
            "| Metric                      | Full Retrain (M0)   |   Fisher Scrub (M2) |\n",
            "+=============================+=====================+=====================+\n",
            "| Retain Acc (%) (Digits 1-9) | 98.91               |               95.63 |\n",
            "+-----------------------------+---------------------+---------------------+\n",
            "| Forget Acc (%) (Digit 0)    | 0.00                |                0.03 |\n",
            "+-----------------------------+---------------------+---------------------+\n",
            "| Weight Dist (L2)            | 0.0 (Ref)           |               39.82 |\n",
            "+-----------------------------+---------------------+---------------------+\n",
            "| Runtime (s)                 | 53.53               |                1.6  |\n",
            "+-----------------------------+---------------------+---------------------+\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import copy\n",
        "import time\n",
        "from tabulate import tabulate\n",
        "\n",
        "CONFIG = {\n",
        "    \"batch_size\": 128,\n",
        "    \"lr\": 0.01,\n",
        "    \"epochs_m0\": 5,\n",
        "    \"unlearn_epochs\": 1,\n",
        "\n",
        "    # Aggressive settings are allowed here because the gradients\n",
        "    # for \"0\" are distinct from \"1-9\".\n",
        "    \"unlearn_lr\": 0.02,   \n",
        "    \"fisher_samples\": 1000, \n",
        "    \"alpha\": 0.5,           \n",
        "    \"max_scrub_steps\": 50,  \n",
        "    \"total_users\": 100,\n",
        "    \"zipf_param\": 1.5, # (Ignored for this specific setup)\n",
        "    \"seed\": 42,\n",
        "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "}\n",
        "\n",
        "torch.manual_seed(CONFIG[\"seed\"])\n",
        "np.random.seed(CONFIG[\"seed\"])\n",
        "print(f\"Running on device: {CONFIG['device']}\")\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "def compute_fisher_diagonal(model, loader, num_samples=500):\n",
        "    fisher_diag = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        fisher_diag[name] = torch.zeros_like(param)\n",
        "\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    samples_seen = 0\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(CONFIG[\"device\"]), labels.to(CONFIG[\"device\"])\n",
        "        model.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                fisher_diag[name] += param.grad.data ** 2\n",
        "\n",
        "        samples_seen += inputs.size(0)\n",
        "        if samples_seen >= num_samples:\n",
        "            break\n",
        "\n",
        "    for name in fisher_diag:\n",
        "        fisher_diag[name] /= samples_seen\n",
        "\n",
        "    return fisher_diag\n",
        "\n",
        "def fisher_scrubbing_step(model, inputs, labels, fisher_diag):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    # 1. Log Softmax\n",
        "    outputs = F.log_softmax(model(inputs), dim=1)\n",
        "\n",
        "    # 2. Target: Uniform Distribution (Confusion)\n",
        "    # We want the model to have NO IDEA that these are zeros.\n",
        "    batch_size = inputs.size(0)\n",
        "    num_classes = 10\n",
        "    uniform_target = torch.full((batch_size, num_classes), 1.0 / num_classes).to(CONFIG[\"device\"])\n",
        "\n",
        "    # 3. KL Divergence Loss\n",
        "    criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "    loss = criterion(outputs, uniform_target)\n",
        "\n",
        "    # 4. Backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Update\n",
        "    with torch.no_grad():\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                F_ii = fisher_diag[name]\n",
        "                scale = 1.0 / (F_ii + CONFIG[\"alpha\"])\n",
        "\n",
        "                # Subtract gradient (Minimize KL)\n",
        "                update = CONFIG[\"unlearn_lr\"] * scale * param.grad\n",
        "                update.clamp_(min=-0.05, max=0.05)\n",
        "\n",
        "                param.sub_(update)\n",
        "\n",
        "def run_training(model, loader, epochs):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=CONFIG[\"lr\"], momentum=0.9)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(CONFIG[\"device\"]), labels.to(CONFIG[\"device\"])\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(inputs), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0; total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(CONFIG[\"device\"]), labels.to(CONFIG[\"device\"])\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    if total == 0: return 0.0\n",
        "    return (100 * correct / total)\n",
        "\n",
        "def get_model_dist(model_a, model_b):\n",
        "    dist = 0.0\n",
        "    for p1, p2 in zip(model_a.parameters(), model_b.parameters()):\n",
        "        dist += torch.norm(p1 - p2, p=2).item()\n",
        "    return dist\n",
        "\n",
        "# DATA SETUP (here we have a special setup for how data get splited)\n",
        "\n",
        "print(\"Loading MNIST...\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "full_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "print(\"Constructing Class-Split (User 0 = Digit 0)...\")\n",
        "\n",
        "# Identify indices\n",
        "targets = full_dataset.targets\n",
        "forget_indices = (targets == 0).nonzero(as_tuple=True)[0].tolist()\n",
        "retain_indices_pool = (targets != 0).nonzero(as_tuple=True)[0].tolist()\n",
        "\n",
        "data_map = {}\n",
        "\n",
        "# User 0 gets ALL Zeros\n",
        "data_map[0] = forget_indices\n",
        "\n",
        "# Distribute Digits 1-9 among other users (randomly; no one-to-one mapping)\n",
        "remaining_users = CONFIG[\"total_users\"] - 1\n",
        "imgs_per_user = len(retain_indices_pool) // remaining_users\n",
        "\n",
        "curr_idx = 0\n",
        "for u in range(1, CONFIG[\"total_users\"]):\n",
        "    start = curr_idx\n",
        "    end = curr_idx + imgs_per_user\n",
        "    if u == CONFIG[\"total_users\"] - 1:\n",
        "        data_map[u] = retain_indices_pool[start:]\n",
        "    else:\n",
        "        data_map[u] = retain_indices_pool[start:end]\n",
        "    curr_idx += imgs_per_user\n",
        "\n",
        "print(f\"User 0 (Forget Target) has {len(data_map[0])} images (All Zeros)\")\n",
        "print(f\"Other Users share {len(retain_indices_pool)} images (Digits 1-9)\")\n",
        "\n",
        "# generate result tables \n",
        "print(\"\\n--- [Phase 1] Generating Table 1 ---\")\n",
        "\n",
        "# 1. Train Base Model (All Digits 0-9)\n",
        "print(\"Training Base Model (M_All)...\")\n",
        "base_loader = DataLoader(full_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
        "base_model = SimpleMLP().to(CONFIG[\"device\"])\n",
        "run_training(base_model, base_loader, CONFIG[\"epochs_m0\"])\n",
        "\n",
        "# Setup Loaders\n",
        "target_user = 0\n",
        "forget_idx = data_map[target_user]\n",
        "retain_idx = [i for i in range(len(full_dataset)) if i not in forget_idx]\n",
        "\n",
        "retain_loader = DataLoader(Subset(full_dataset, retain_idx), batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
        "forget_loader = DataLoader(Subset(full_dataset, forget_idx), batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
        "\n",
        "# 2. Train M0 (Retrain on only Digits 1-9)\n",
        "print(\"Running Retrain Benchmark (M0 - No Zeros)...\")\n",
        "start = time.time()\n",
        "m0_model = SimpleMLP().to(CONFIG[\"device\"])\n",
        "run_training(m0_model, retain_loader, CONFIG[\"epochs_m0\"])\n",
        "time_m0 = time.time() - start\n",
        "\n",
        "# 3. Train M2 (Fisher Scrubbing to remove Zeros)\n",
        "print(\"Running Fisher Scrub (M2)...\")\n",
        "start = time.time()\n",
        "m2_model = copy.deepcopy(base_model)\n",
        "fisher_diag = compute_fisher_diagonal(m2_model, retain_loader, CONFIG[\"fisher_samples\"])\n",
        "\n",
        "scrub_steps = 0\n",
        "for _ in range(CONFIG[\"unlearn_epochs\"]):\n",
        "    for inputs, labels in forget_loader:\n",
        "        fisher_scrubbing_step(m2_model, inputs.to(CONFIG[\"device\"]), labels.to(CONFIG[\"device\"]), fisher_diag)\n",
        "        scrub_steps += 1\n",
        "        if scrub_steps >= CONFIG[\"max_scrub_steps\"]:\n",
        "            break\n",
        "    if scrub_steps >= CONFIG[\"max_scrub_steps\"]:\n",
        "        break\n",
        "time_m2 = time.time() - start\n",
        "\n",
        "# Metrics\n",
        "# Note: M0 Acc on Forget Set should be ~0.0% (It never saw a zero)\n",
        "acc_m0_r = evaluate(m0_model, retain_loader)\n",
        "acc_m2_r = evaluate(m2_model, retain_loader)\n",
        "acc_m0_f = evaluate(m0_model, forget_loader)\n",
        "acc_m2_f = evaluate(m2_model, forget_loader)\n",
        "w_dist = get_model_dist(m0_model, m2_model)\n",
        "\n",
        "table_data = [\n",
        "    [\"Metric\", \"Full Retrain (M0)\", \"Fisher Scrub (M2)\"],\n",
        "    [\"Retain Acc (%) (Digits 1-9)\", f\"{acc_m0_r:.2f}\", f\"{acc_m2_r:.2f}\"],\n",
        "    [\"Forget Acc (%) (Digit 0)\", f\"{acc_m0_f:.2f}\", f\"{acc_m2_f:.2f}\"],\n",
        "    [\"Weight Dist (L2)\", \"0.0 (Ref)\", f\"{w_dist:.2f}\"],\n",
        "    [\"Runtime (s)\", f\"{time_m0:.2f}\", f\"{time_m2:.2f}\"]\n",
        "]\n",
        "print(\"\\n\" + tabulate(table_data, headers=\"firstrow\", tablefmt=\"grid\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
